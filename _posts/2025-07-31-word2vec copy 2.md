---
layout: post
title: Word2vec
subtitle: 
tags: [tech]
---
I am trying to find some tools for a lexical semantic change analysis, and bumped into this word2vec thingy that looks bit fun. I am looking at its instructions and hope to be able to run some code in a couple days. 

This was published more than 10 years ago, and is an embedding service that’s a byproduct of the deep learning mechanism. I thought about learning the much fancier tools like Bert or whatnot, but firstly, I don’t understand Bert enough to use it. I skimmed some instructions and figured it might take a while before I can jump into it. Secondly, I have a problem that asks for a specific tool, I cannot think of a reason to learn how to drive a plane in order to operate a wheelchair. 

My idea is to find slices of corpora and train vectors diachronically for three to five very specific words. I cannot think of an explanation to validify my choice of the five words, but I guess it’s fine. I am not perfect. I guess I have some rationales but they are not gonna be perfect either. 

The idea of word to vec is to use a basic model, giving it enough data so that it fine-tunes its parameters, and then test it with another set of data to see if it’s done a good job. When it’s prediction is good enough, we can strip the vectors out of the words and read the numbers. 

My research question in this way looks pretty lame. It doesn’t really answer any of the big philosophical questions. It’s like playing with a toy and see how to play it with some tricks. But meh, it’s a bit fun. 